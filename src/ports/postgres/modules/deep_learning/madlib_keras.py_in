# coding=utf-8
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

import datetime
import numpy as np
import os
import plpy
import sys
import time

# Do not remove `import keras` although it's not directly used in this file.
# For ex if the user passes in the optimizer as keras.optimizers.SGD instead of just
# SGD, then without this import this python file won't find the SGD module
import keras

from keras import backend as K
from keras import utils as keras_utils
from keras.layers import *
from keras.models import *
from keras.optimizers import *
from keras.regularizers import *
import madlib_keras_serializer
from madlib_keras_helper import CLASS_VALUES_COLNAME
from madlib_keras_helper import DEPENDENT_VARTYPE_COLNAME
from madlib_keras_helper import expand_input_dims
from madlib_keras_helper import NORMALIZING_CONST_COLNAME
from madlib_keras_validator import FitInputValidator
from madlib_keras_wrapper import *
from keras_model_arch_table import Format

from utilities.model_arch_info import get_input_shape
from utilities.model_arch_info import get_num_classes
from utilities.utilities import is_platform_pg
from utilities.utilities import madlib_version
from utilities.validate_args import get_col_value_and_type
from utilities.validate_args import quote_ident

def fit(schema_madlib, source_table, model, dependent_varname,
        independent_varname, model_arch_table, model_arch_id, compile_params,
        fit_params, num_iterations, use_gpu = True,
        validation_table=None, name="", description="", **kwargs):

    source_table = quote_ident(source_table)
    dependent_varname = quote_ident(dependent_varname)
    independent_varname = quote_ident(independent_varname)
    model_arch_table = quote_ident(model_arch_table)

    fit_validator = FitInputValidator(
        source_table, validation_table, model, model_arch_table,
        dependent_varname, independent_varname, num_iterations)

    start_training_time = datetime.datetime.now()
    use_gpu = bool(use_gpu)

    # Get the serialized master model
    start_deserialization = time.time()
    model_arch_query = "SELECT {0}, {1} FROM {2} WHERE {3} = {4}".format(
                                        Format.MODEL_ARCH, Format.MODEL_WEIGHTS,
                                        model_arch_table, Format.MODEL_ID,
                                        model_arch_id)
    query_result = plpy.execute(model_arch_query)
    if not  query_result:
        plpy.error("no model arch found in table {0} with id {1}".format(
            model_arch_table, model_arch_id))
    query_result = query_result[0]
    model_arch = query_result[Format.MODEL_ARCH]
    input_shape = get_input_shape(model_arch)
    num_classes = get_num_classes(model_arch)
    fit_validator.validate_input_shapes(input_shape)
    model_weights_serialized = query_result[Format.MODEL_WEIGHTS]

    #TODO: Refactor the pg related logic in a future PR when we think
    # about making the fit function easier to read and maintain.
    if is_platform_pg():
        set_keras_session(use_gpu)
    else:
        # Disable GPU on master for gpdb
        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'

    # Compute total images on each segment
    gp_segment_id_col,\
    seg_ids_train,\
    images_per_seg_train = get_images_per_seg(source_table, dependent_varname)

    if not all(images_per_seg_train):
        plpy.error("No images in training dataset!  Aborting")

    if validation_table:
        _, seg_ids_val,\
        images_per_seg_val = get_images_per_seg(validation_table, dependent_varname)

    # Convert model from json and initialize weights
    master_model = model_from_json(model_arch)
    model_weights = master_model.get_weights()

    # Get shape of weights in each layer from model arch
    model_shapes = []
    for weight_arr in master_model.get_weights():
        model_shapes.append(weight_arr.shape)

    if model_weights_serialized:
        # If warm start from previously trained model, set weights
        model_weights = madlib_keras_serializer.deserialize_weights_orig(
            model_weights_serialized, model_shapes)
        master_model.set_weights(model_weights)

    # Construct validation dataset if provided
    validation_set_provided = bool(validation_table)
    validation_aggregate_accuracy = []; validation_aggregate_loss = []

    # Prepare the SQL for running distributed training via UDA
    compile_params_to_pass = "$madlib$" + compile_params + "$madlib$"
    fit_params_to_pass = "$madlib$" + fit_params + "$madlib$"
    run_training_iteration = plpy.prepare("""
        SELECT {schema_madlib}.fit_step(
            {independent_varname}::REAL[],
            {dependent_varname}::SMALLINT[],
            {gp_segment_id_col},
            {num_classes}::INTEGER,
            ARRAY{seg_ids_train},
            ARRAY{images_per_seg_train},
            $MAD${model_arch}$MAD$::TEXT,
            {compile_params_to_pass}::TEXT,
            {fit_params_to_pass}::TEXT,
            {use_gpu},
            $1
        ) AS iteration_result
        FROM {source_table}
        """.format(**locals()), ["bytea"])

    # Define the state for the model and loss/accuracy storage lists
    model_state = madlib_keras_serializer.serialize_weights(
        0, 0, 0, model_weights)
    aggregate_loss, aggregate_accuracy, aggregate_runtime = [], [], []

    plpy.info("Model architecture size: {}KB".format(len(model_arch)/1024))
    plpy.info("Model state (serialized) size: {}MB".format(
        len(model_state)/1024/1024))

    # Run distributed training for specified number of iterations
    for i in range(num_iterations):
        start_iteration = time.time()
        iteration_result = plpy.execute(run_training_iteration, [model_state])[0]['iteration_result']
        end_iteration = time.time()
        plpy.info("Time for iteration {0}: {1} sec".
                  format(i + 1, end_iteration - start_iteration))
        aggregate_runtime.append(datetime.datetime.now())
        avg_loss, avg_accuracy, model_state = madlib_keras_serializer.deserialize_iteration_state(iteration_result)
        plpy.info("Average loss after training iteration {0}: {1}".format(
            i + 1, avg_loss))
        plpy.info("Average accuracy after training iteration {0}: {1}".format(
            i + 1, avg_accuracy))
        if validation_set_provided:
            _, _, _, updated_weights = madlib_keras_serializer.deserialize_weights(
                model_state, model_shapes)
            master_model.set_weights(updated_weights)
            start_val = time.time()
            evaluate_result = get_loss_acc_from_keras_eval(schema_madlib,
                                                           validation_table,
                                                           dependent_varname,
                                                           independent_varname,
                                                           compile_params_to_pass,
                                                           model_arch, model_state,
                                                           use_gpu, seg_ids_val,
                                                           images_per_seg_val,
                                                           gp_segment_id_col)
            end_val = time.time()
            plpy.info("Time for validation in iteration {0}: {1} sec". format(i + 1, end_val - start_val))
            if len(evaluate_result) < 2:
                plpy.error('Calling evaluate on validation data returned < 2 '
                           'metrics. Expected metrics are loss and accuracy')
            validation_loss = evaluate_result[0]
            validation_accuracy = evaluate_result[1]
            plpy.info("Validation set accuracy after iteration {0}: {1}".
                      format(i + 1, validation_accuracy))
            validation_aggregate_accuracy.append(validation_accuracy)
            validation_aggregate_loss.append(validation_loss)
        aggregate_loss.append(avg_loss)
        aggregate_accuracy.append(avg_accuracy)

    end_training_time = datetime.datetime.now()

    final_validation_acc = None
    if validation_aggregate_accuracy and len(validation_aggregate_accuracy) > 0:
        final_validation_acc = validation_aggregate_accuracy[-1]

    final_validation_loss = None
    if validation_aggregate_loss and len(validation_aggregate_loss) > 0:
        final_validation_loss = validation_aggregate_loss[-1]
    version = madlib_version(schema_madlib)
    class_values, class_values_type = get_col_value_and_type(
        fit_validator.source_summary_table, CLASS_VALUES_COLNAME)
    norm_const, norm_const_type = get_col_value_and_type(
        fit_validator.source_summary_table, NORMALIZING_CONST_COLNAME)
    dep_vartype = plpy.execute("SELECT {0} AS dep FROM {1}".format(
        DEPENDENT_VARTYPE_COLNAME, fit_validator.source_summary_table))[0]['dep']
    create_output_summary_table = plpy.prepare("""
        CREATE TABLE {0}_summary AS
        SELECT
        $1 AS model_arch_table,
        $2 AS model_arch_id,
        $3 AS model_type,
        $4 AS start_training_time,
        $5 AS end_training_time,
        $6 AS source_table,
        $7 AS validation_table,
        $8 AS model,
        $9 AS dependent_varname,
        $10 AS independent_varname,
        $11 AS name,
        $12 AS description,
        $13 AS model_size,
        $14 AS madlib_version,
        $15 AS compile_params,
        $16 AS fit_params,
        $17 AS num_iterations,
        $18 AS num_classes,
        $19 AS accuracy,
        $20 AS loss,
        $21 AS accuracy_iter,
        $22 AS loss_iter,
        $23 AS time_iter,
        $24 AS accuracy_validation,
        $25 AS loss_validation,
        $26 AS accuracy_iter_validation,
        $27 AS loss_iter_validation,
        $28 AS {1},
        $29 AS {2},
        $30 AS {3}
        """.format(model, CLASS_VALUES_COLNAME, DEPENDENT_VARTYPE_COLNAME,
                   NORMALIZING_CONST_COLNAME),
                   ["TEXT", "INTEGER", "TEXT", "TIMESTAMP",
                    "TIMESTAMP", "TEXT", "TEXT","TEXT",
                    "TEXT", "TEXT", "TEXT", "TEXT", "INTEGER",
                    "TEXT", "TEXT", "TEXT", "INTEGER",
                    "INTEGER", "DOUBLE PRECISION",
                    "DOUBLE PRECISION", "DOUBLE PRECISION[]",
                    "DOUBLE PRECISION[]", "TIMESTAMP[]",
                    "DOUBLE PRECISION", "DOUBLE PRECISION",
                    "DOUBLE PRECISION[]", "DOUBLE PRECISION[]",
                    class_values_type, "TEXT", norm_const_type])
    plpy.execute(
        create_output_summary_table,
        [
            model_arch_table, model_arch_id,
            "madlib_keras",
            start_training_time, end_training_time,
            source_table, validation_table,
            model, dependent_varname,
            independent_varname, name, description,
            sys.getsizeof(model), version, compile_params,
            fit_params, num_iterations, num_classes,
            aggregate_accuracy[-1],
            aggregate_loss[-1],
            aggregate_accuracy, aggregate_loss,
            aggregate_runtime, final_validation_acc,
            final_validation_loss,
            validation_aggregate_accuracy,
            validation_aggregate_loss,
            class_values,
            dep_vartype,
            norm_const
        ]
        )

    create_output_table = plpy.prepare("""
        CREATE TABLE {0} AS
        SELECT $1 as model_data""".format(model), ["bytea"])
    plpy.execute(create_output_table, [model_state])

    if is_platform_pg():
        clear_keras_session()

def get_images_per_seg(source_table, dependent_varname):
    """
    Compute total images in each segment, by querying source_table.  For
    postgres, this is just the total number of images in the db.
    :param source_table:
    :param dependent_var:
    :return: Returns a string and two arrays
    1. The appropriate string to use for querying segment number
    ("gp_segment_id" for gpdb or "-1" for postgres).
    1. An array containing all the segment numbers in ascending order
    1. An array containing the total images on each of the segments in the
    segment array.
    """
    if is_platform_pg():
        images_per_seg = plpy.execute(
            """ SELECT SUM(ARRAY_LENGTH({0}, 1)) AS images_per_seg
                FROM {1}
            """.format(dependent_varname, source_table))
        seg_ids = [0]
        gp_segment_id_col = -1
    else:
        images_per_seg = plpy.execute(
            """ SELECT gp_segment_id, SUM(ARRAY_LENGTH({0}, 1)) AS images_per_seg
                FROM {1}
                GROUP BY gp_segment_id
            """.format(dependent_varname, source_table))
        seg_ids = [int(each_segment["gp_segment_id"])
                   for each_segment in images_per_seg]
        images_per_seg = [int(each_segment["images_per_seg"])
            for each_segment in images_per_seg]

        gp_segment_id_col = 'gp_segment_id'
    return gp_segment_id_col, seg_ids, images_per_seg

def fit_transition(state, ind_var, dep_var, current_seg_id, num_classes,
                   seg_ids, images_per_seg, architecture,
                   compile_params, fit_params, use_gpu, previous_state,
                   **kwargs):

    """

    :param state:
    :param ind_var:
    :param dep_var:
    :param current_seg_id:
    :param num_classes:
    :param seg_ids:
    :param images_per_seg:
    :param architecture:
    :param compile_params:
    :param fit_params:
    :param use_gpu:
    :param previous_state:
    :param kwargs:
    :return:
    """
    if not ind_var or not dep_var:
        return state

    start_transition = time.time()
    SD = kwargs['SD']
    # Configure GPUs/CPUs
    device_name = get_device_name_and_set_cuda_env(use_gpu, current_seg_id)

    # Set up system if this is the first buffer on segment'

    if not state:
        if not is_platform_pg():
            set_keras_session(use_gpu)
        segment_model = model_from_json(architecture)
        SD['model_shapes'] = madlib_keras_serializer.get_model_shapes(segment_model)
        compile_and_set_weights(segment_model, compile_params, device_name,
                                previous_state, SD['model_shapes'])
        SD['segment_model'] = segment_model
        image_count = 0
        agg_loss = 0
        agg_accuracy = 0
        agg_image_count = 0
    else:
        segment_model = SD['segment_model']
        #TODO we don't need to deserialize the weights here.
        agg_loss, agg_accuracy, agg_image_count, _ = madlib_keras_serializer.deserialize_weights(
            state, SD['model_shapes'])

    # Prepare the data
    x_train = np.array(ind_var, dtype='float64')
    y_train = np.array(dep_var)

    # Fit segment model on data
    start_fit = time.time()
    with K.tf.device(device_name):
        #TODO consider not doing this every time
        fit_params = parse_and_validate_fit_params(fit_params)
        history = segment_model.fit(x_train, y_train, **fit_params)
        loss = history.history['loss'][0]
        accuracy = history.history['acc'][0]
    end_fit = time.time()

    image_count = len(x_train)
    # Aggregating number of images, loss and accuracy
    agg_image_count += image_count
    agg_loss += (image_count * loss)
    agg_accuracy += (image_count * accuracy)

    with K.tf.device(device_name):
        updated_weights = segment_model.get_weights()
    if is_platform_pg():
        total_images = images_per_seg[0]
    else:
        total_images = images_per_seg[seg_ids.index(current_seg_id)]
    if total_images == 0:
        plpy.error('Got 0 rows. Expected at least 1.')

    # Re-serialize the weights
    # Update image count, check if we are done
    if agg_image_count == total_images:
        if total_images == 0:
            plpy.error('Total images is 0')
        # Once done with all images on a segment, we update weights
        # with the total number of images here instead of the merge function.
        # The merge function only deals with aggregating them.
        updated_weights = [ total_images * w for w in updated_weights ]
        if not is_platform_pg():
            # In GPDB, each segment would have a keras session, so clear
            # them after the last buffer is processed.
            clear_keras_session()
    elif agg_image_count > total_images:
        plpy.error('Processed {0} images, but there were supposed to be only {1}!'
            .format(agg_image_count,total_images))

    new_model_state = madlib_keras_serializer.serialize_weights(
            agg_loss, agg_accuracy, agg_image_count, updated_weights)


    del x_train
    del y_train

    end_transition = time.time()
    plpy.info("Processed {0} images: Fit took {1} sec, Total was {2} sec".format(
        image_count, end_fit - start_fit, end_transition - start_transition))

    return new_model_state

def fit_merge(state1, state2, **kwargs):

    # Return if called early
    if not state1 or not state2:
        return state1 or state2

    # Deserialize states
    loss1, accuracy1, image_count1, weights1 = madlib_keras_serializer.deserialize_weights_merge(state1)
    loss2, accuracy2, image_count2, weights2 = madlib_keras_serializer.deserialize_weights_merge(state2)

    # Compute total image counts
    image_count = (image_count1 + image_count2) * 1.0

    # Aggregate the losses
    total_loss = loss1 + loss2

    # Aggregate the accuracies
    total_accuracy = accuracy1 + accuracy2

    # Aggregate the weights
    total_weights = weights1 + weights2

    # Return the merged state
    return madlib_keras_serializer.serialize_weights_merge(
        total_loss, total_accuracy, image_count, total_weights)

def fit_final(state, **kwargs):
    # Return if called early
    if not state:
        return state

    loss, accuracy, image_count, weights = madlib_keras_serializer.deserialize_weights_merge(state)
    if image_count == 0:
        plpy.error("fit_final: Total images processed is 0")

    # Averaging the accuracy, loss and weights
    loss /= image_count
    accuracy /= image_count
    weights /= image_count
    return madlib_keras_serializer.serialize_weights_merge(
        loss, accuracy, image_count, weights)

def evaluate1(schema_madlib, model_table, test_table, id_col, model_arch_table,
            model_arch_id, dependent_varname, independent_varname,
            compile_params, output_table, **kwargs):
    # module_name = 'madlib_keras_evaluate'
    # input_tbl_valid(test_table, module_name)
    # input_tbl_valid(model_arch_table, module_name)
    # output_tbl_valid(output_table, module_name)

    # _validate_input_args(test_table, model_arch_table, output_table)

    model_data_query = "SELECT model_data from {0}".format(model_table)
    model_data = plpy.execute(model_data_query)[0]['model_data']

    model_arch_query = "SELECT model_arch, model_weights FROM {0} " \
                       "WHERE id = {1}".format(model_arch_table, model_arch_id)
    query_result = plpy.execute(model_arch_query)
    if not  query_result or len(query_result) == 0:
        plpy.error("no model arch found in table {0} with id {1}".format(
            model_arch_table, model_arch_id))
    query_result = query_result[0]
    model_arch = query_result[Format.MODEL_ARCH]
    compile_params = "$madlib$" + compile_params + "$madlib$"

    loss_acc = get_loss_acc_from_keras_eval(schema_madlib, test_table, dependent_varname,
                                            independent_varname, compile_params, model_arch,
                                            model_data, False)
    #TODO remove these infos after adding create table command
    plpy.info('len of evaluate result is {}'.format(len(loss_acc)))
    plpy.info('evaluate result loss is {}'.format(loss_acc[0]))
    plpy.info('evaluate result acc is {}'.format(loss_acc[1]))

def get_loss_acc_from_keras_eval(schema_madlib, table, dependent_varname,
                                 independent_varname, compile_params,
                                 model_arch, model_data, use_gpu,
                                 seg_ids, images_per_seg, gp_segment_id_col):
    """
    This function will call the internal keras evaluate function to get the loss
    and accuracy of each tuple which then gets averaged to get the final result.
    """
    evaluate_query = plpy.prepare("""
    -- TODO:  really, we should not be casting integers and big integers to smallint's
    --  The right solution is either to change the datatype of the agg function from
    --  SMALLINT to INTEGER, or change the output of minibatch util to produce SMALLINT
    --  For the first, we should change fit_step also
    select ({schema_madlib}.internal_keras_evaluate({dependent_varname}::SMALLINT[],
                                            {independent_varname}::REAL[],
                                            $MAD${model_arch}$MAD$,
                                            $1,
                                            {compile_params},
                                            {use_gpu},
                                            ARRAY{seg_ids},
                                            ARRAY{images_per_seg},
                                            {gp_segment_id_col})) as loss_acc
        from {table}
    """.format(**locals()), ["bytea"])
    res = plpy.execute(evaluate_query, [model_data])
    loss_acc = res[0]['loss_acc']
    return loss_acc

def internal_keras_eval_transition(state, dependent_var, independent_var, model_architecture,
                            model_data, compile_params, use_gpu, seg_ids,
                            images_per_seg, current_seg_id, **kwargs):
    SD = kwargs['SD']
    device_name = get_device_name_and_set_cuda_env(use_gpu, current_seg_id)

    agg_loss, agg_accuracy, agg_image_count = state

    if not agg_image_count:
        if not is_platform_pg():
            set_keras_session(use_gpu)
        model = model_from_json(model_architecture)
        model_shapes = madlib_keras_serializer.get_model_shapes(model)
        _, _, _, model_weights = madlib_keras_serializer.deserialize_weights(
                model_data, model_shapes)
        model.set_weights(model_weights)
        with K.tf.device(device_name):
            compile_model(model, compile_params)
        SD['segment_model'] = model
        # These should already be 0, but just in case make sure
        agg_accuracy = 0
        agg_loss = 0
    else:
        # Same model every time, no need to re-compile or update weights
        model = SD['segment_model']

    x_val = np.array(independent_var)
    y_val = np.array(dependent_var)

    with K.tf.device(device_name):
        res = model.evaluate(x_val, y_val)

    loss, accuracy = res

    image_count = len(dependent_var)

    agg_image_count += image_count
    agg_loss += (image_count * loss)
    agg_accuracy += (image_count * accuracy)

    if is_platform_pg():
        total_images = images_per_seg[0]
    else:
        total_images = images_per_seg[seg_ids.index(current_seg_id)]

    if agg_image_count == total_images:
        SD.pop('segment_model', None)
        if not is_platform_pg():
            clear_keras_session()
    elif agg_image_count > total_images:
        plpy.error("Evaluated too many images.")

    state[0] = agg_loss
    state[1] = agg_accuracy
    state[2] = agg_image_count

    return state

def internal_keras_eval_merge(state1, state2, **kwargs):
    # If either state is None, return the other one
    if not state1 or not state2:
        return state1 or state2

    loss1, accuracy1, image_count1 = state1
    loss2, accuracy2, image_count2 = state2

    merged_loss = loss1 + loss2
    merged_accuracy = accuracy1 + accuracy2

    total_image_count = image_count1 + image_count2

    merged_state = [ merged_loss, merged_accuracy , total_image_count ]

    return merged_state

def internal_keras_eval_final(state, **kwargs):
    loss, accuracy, image_count = state

    if image_count == 0:
        plpy.error("internal_keras_eval_final: Total images processed is 0")

    loss /= image_count
    accuracy /= image_count

    state = loss, accuracy, image_count
    return state
